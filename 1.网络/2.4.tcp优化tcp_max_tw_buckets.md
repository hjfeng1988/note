# 优化tcp_max_tw_buckets

场景1：假设架构client --> server，都是单台

### 客户端主动关闭，TIME_WAIT产生在客户端
* 如果tcp_max_tw_buckets > ip_local_port_range，客户端会先触发ip_local_port_range，客户端无法创建新连接
* 如果tcp_max_tw_buckets < ip_local_port_range，客户端会先触发tcp_max_tw_buckets，客户端跳过TIME_WAIT，直接进入CLOSED，假设客户端挥手的第4次ACK丢失，服务端停留在LAST_ACK
  * 服务端重传挥手期间的FIN，客户端向服务端回应RST
  * 客户端在服务端重传FIN之前，发起了同样四元组的新连接，服务端检查双方的timestamps
    * 如果已启用，客户端向服务端回应RST
    * 如果未启用，服务端向客户端回应RST
  * RST之后，又发起了同样四元组的新连接，可能接收到延迟的数据包，造成数据错乱


### 服务端主动关闭，TIME_WAIT产生在服务端
* 与服务端ip_local_port_range无关
* 如果服务端触发tcp_max_tw_buckets，跳过TIME_WAIT，直接进入CLOSED，假设服务端第4次ACK丢失，客户端停留在LAST_ACK
  * 客户端重传挥手期间的FIN，服务端向客户端回应RST
  * 服务端不会主动向客户端发起请求，不存在其他问题

---
场景2：假设架构是client --> proxy --> upstream
>一般短连接出现在proxy --> upstream

### proxy主动关闭，TIME_WAIT产生在代理端

* 代理端先触发ip_local_port_range，无法创建新连接，客户端接收502
* 代理端先触发tcp_max_tw_buckets，跳过TIME_WAIT，直接进入CLOSED
  * 代理端、客户端可能接收到延迟的重复包，对新连接造成数据错乱
  * 上游服务器可能无法优雅关闭


### 客户端或代理端无法创建新连接的优化方案
* 改用长连接
* 增加ip_local_port_range，减少tcp_max_tw_buckets
* 增加proxy实例数、upstream实例数
* 保证`tcp_max_tw_buckets < upstream实例数 * ip_local_port_range`，tcp_max_tw_buckets上限时，跳过TIME_WAIT，引发上游服务器不优雅关闭，客户端或代理端可能收到延迟的数据包
* 启用tcp_tw_reuse和tcp_timestamps，引发上游服务器不优雅关闭
