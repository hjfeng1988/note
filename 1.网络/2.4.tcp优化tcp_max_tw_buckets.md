# 优化tcp_max_tw_buckets

场景1：假设架构client --> server，都是单台

### 客户端主动关闭，TIME_WAIT产生在客户端
* 如果tcp_max_tw_buckets > ip_local_port_range，客户端会先触发ip_local_port_range，客户端无法创建新连接
* 如果tcp_max_tw_buckets < ip_local_port_range，客户端会先触发tcp_max_tw_buckets，客户端跳过TIME_WAIT，直接进入CLOSED，假设客户端挥手的第4次ACK丢失，服务端停留在LAST_ACK
  * 客户端可能接收到延迟的重复包，对新连接造成数据错乱
  * 服务端重传挥手期间的FIN，客户端回应RST
  * 客户端早于服务端重传FIN，发起了同样四元组的新连接，服务端查看未启用timestamps，服务端回应RST
  * 客户端早于服务端重传FIN，发起了同样四元组的新连接，服务端查看已启用timestamps，服务端回复挥手期间的FIN，客户端回应RST


### 服务端主动关闭，TIME_WAIT产生在服务端
* 与服务端ip_local_port_range无关
* 如果服务端触发tcp_max_tw_buckets，跳过TIME_WAIT，直接进入CLOSED，假设服务端第4次ACK丢失，客户端停留在LAST_ACK
  * 客户端重传挥手期间的FIN，服务端回应RST
  * 服务端不会主动向客户端发起请求，不存在其他问题

---
场景2：假设架构是client --> proxy --> upstream，一般短连接出现在proxy --> upstream

### proxy主动关闭，TIME_WAIT产生在代理端

* 代理端先触发ip_local_port_range，无法创建新连接，客户端接收50x
* 代理端先触发tcp_max_tw_buckets，跳过TIME_WAIT，直接进入CLOSED
  * 代理端、客户端可能接收到延迟的重复包，对新连接造成数据错乱
  * 上游服务器可能无法优雅关闭


### 如何解决ip_local_port_range不够用
* 改用长连接
* 增加proxy实例数、ip_local_port_range、tcp_max_tw_buckets、upstream实例数，保证`tcp_max_tw_buckets < upstream实例数 * ip_local_port_range`，tcp_max_tw_buckets上限时，跳过TIME_WAIT，引发上游服务器不优雅关闭，客户端可能收到延迟的重复包
* 启用tw_reuse和tcp_timestamps，引发上游服务器不优雅关闭
